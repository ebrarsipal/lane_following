ğŸš— PekiÅŸtirmeli Ã–ÄŸrenme ile Åerit Takibi (RL Lane Following)
Bu proje, bir aracÄ± dairesel bir pistte ÅŸeridini takip etmesi iÃ§in eÄŸitmeyi amaÃ§layan bir PekiÅŸtirmeli Ã–ÄŸrenme (Reinforcement Learning - RL) simÃ¼lasyonudur. Åerit takibi problemi, Derin Q AÄŸÄ± (Deep Q-Network - DQN) algoritmasÄ± kullanÄ±larak Ã§Ã¶zÃ¼lmÃ¼ÅŸtÃ¼r.

ğŸŒŸ Temel Ã–zellikler
Ortam: Ã–zel olarak tasarlanmÄ±ÅŸ, sÃ¼rekli durum (Continuous State) ve ayrÄ±k eylem (Discrete Action) alanÄ±na sahip dairesel ÅŸerit takip ortamÄ± (LaneFollowingCircleEnv).

Algoritma: Model tabanlÄ± olmayan, deÄŸer tabanlÄ± Ã¶ÄŸrenme algoritmasÄ± olan DQN kullanÄ±lmÄ±ÅŸtÄ±r.

GÃ¶rselleÅŸtirme: EÄŸitilmiÅŸ ajanÄ±n performansÄ±nÄ± Streamlit tabanlÄ± bir arayÃ¼z ile gÃ¶rsel olarak izleme imkanÄ±.

Teknolojiler: Python, PyTorch, Streamlit ve Matplotlib.

âš™ï¸ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma
1. Ã–n KoÅŸullar

Bu projeyi yerel ortamÄ±nÄ±zda Ã§alÄ±ÅŸtÄ±rmak iÃ§in aÅŸaÄŸÄ±daki yazÄ±lÄ±mlara ihtiyacÄ±nÄ±z vardÄ±r:

Python 3.8+

Git

2. Depoyu Klonlama

Proje dosyalarÄ±nÄ± yerel makinenize indirin:

Bash
git clone https://github.com/ebrarsipal/lane-following-rl.git
cd rl_lane_following_v1
3. Sanal Ortam OluÅŸturma ve EtkinleÅŸtirme

Proje baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± izole etmek iÃ§in bir sanal ortam oluÅŸturun ve etkinleÅŸtirin:

Bash
# Sanal ortam oluÅŸturma
python -m venv venv

# Windows iÃ§in etkinleÅŸtirme
.\venv\Scripts\activate

# Linux/macOS iÃ§in etkinleÅŸtirme
# source venv/bin/activate 
4. BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleme

Gerekli Python kÃ¼tÃ¼phanelerini yÃ¼kleyin (bir requirements.txt dosyasÄ±nÄ±n mevcut olduÄŸunu varsayar):

Bash
pip install -r requirements.txt
5. Model DosyasÄ±

EÄŸitilmiÅŸ DQN modelinin (dqn_model.pth) projenin ana dizininde bulunduÄŸundan emin olun.

ğŸ–¥ï¸ SimÃ¼lasyonu BaÅŸlatma
AjanÄ±n performansÄ±nÄ± gÃ¶rselleÅŸtirmek iÃ§in Streamlit uygulamasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:

Bash
streamlit run streamlit_app_v2.py
TarayÄ±cÄ±nÄ±z otomatik olarak aÃ§Ä±lacak ve gÃ¶rselleÅŸtirme arayÃ¼zÃ¼nÃ¼ gÃ¶receksiniz.

ArayÃ¼z KullanÄ±mÄ±

Kontrol AlanÄ±	AÃ§Ä±klama
Episodes (BÃ¶lÃ¼mler)	KaÃ§ simÃ¼lasyon bÃ¶lÃ¼mÃ¼ Ã§alÄ±ÅŸtÄ±rmak istediÄŸinizi ayarlar.
Show Trail (Ä°zi GÃ¶ster)	AracÄ±n gittiÄŸi yolu gÃ¶steren izi aÃ§Ä±p kapar.
Start Simulation	EÄŸitilmiÅŸ ajanÄ± ortamda Ã§alÄ±ÅŸtÄ±rmaya baÅŸlar.
SaÄŸ Panel	AnlÄ±k adÄ±m, toplam Ã¶dÃ¼l (reward) ve aracÄ±n konum/baÅŸlÄ±k (heading) bilgilerini gÃ¶sterir.
ğŸ§  Algoritma ve Ortam DetaylarÄ±
Ortam: LaneFollowingCircleEnv

Durum AlanÄ± (State): AracÄ±n ÅŸeritten uzaklÄ±ÄŸÄ±, yola gÃ¶re aÃ§Ä±sÄ± gibi 4 boyutlu sÃ¼rekli vektÃ¶r.

Eylem AlanÄ± (Action): 3 adet ayrÄ±k eylem: Sola dÃ¶n, DÃ¼z Git, SaÄŸa dÃ¶n.

Ã–dÃ¼l (Reward): Åeridin merkezine yakÄ±n kalmak iÃ§in pozitif Ã¶dÃ¼l, pistten sapmak iÃ§in negatif Ã¶dÃ¼l ve ceza.

Ajan: DQNAgent

Bu projede kullanÄ±lan DQN ajanÄ±, Q-tablosunun yerini alan bir sinir aÄŸÄ± (agent.model) kullanÄ±r. EÄŸitilmiÅŸ model, verilen duruma gÃ¶re hangi eylemin en yÃ¼ksek Q deÄŸerine sahip olduÄŸunu belirler ve bu eylemi gerÃ§ekleÅŸtirir.

ğŸ¤ KatkÄ±da Bulunma
Projenin geliÅŸtirilmesine katkÄ±da bulunmaktan memnuniyet duyarÄ±z. LÃ¼tfen bir sorun (Issue) aÃ§maktan veya bir Ã‡ekme Ä°steÄŸi (Pull Request) gÃ¶ndermekten Ã§ekinmeyin.

ğŸ“„ Lisans
Bu proje MIT LisansÄ± altÄ±nda yayÄ±mlanmÄ±ÅŸtÄ±r. (Lisans dosyanÄ±z mevcutsa, daha fazla ayrÄ±ntÄ± iÃ§in LICENSE dosyasÄ±na bakÄ±nÄ±z.)
